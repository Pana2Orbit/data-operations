{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed57d1a4",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b569048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04T18:28:28.829601-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import earthaccess as ea\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from matplotlib.path import Path as PolygonPath\n",
    "from src.services.utils import get_logger\n",
    "from src.services.google import Google\n",
    "\n",
    "logger = get_logger()\n",
    "env_loaded = load_dotenv(find_dotenv())\n",
    "if env_loaded:\n",
    "    logger.info(\"Environment variables loaded successfully.\")\n",
    "else:\n",
    "    logger.error(\"Failed to load environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62856a",
   "metadata": {},
   "source": [
    "## Get data from different sources\n",
    "\n",
    "### Earth data login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52812b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARTH_ACCESS_USERNAME = os.getenv(\"EARTH_ACCESS_USERNAME\")\n",
    "EARTH_ACCESS_PASSWORD = os.getenv(\"EARTH_ACCESS_PASSWORD\")\n",
    "auth = ea.login(EARTH_ACCESS_USERNAME, EARTH_ACCESS_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5106fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name = \"TEMPO_NO2_L3\"  # collection name to search for in the EarthData\n",
    "# short_name = \"OMHCHOd\"\n",
    "version = \"V03\"\n",
    "# version = \"003\"\n",
    "\n",
    "date_start = \"2025-01-01 00:00:00\"\n",
    "date_end = \"2025-01-01 15:59:59\"\n",
    "polygon_coords = [\n",
    "    (-120.0091050, 41.9727325),\n",
    "    (-124.6045661, 41.8898826),\n",
    "    (-120.4462801, 33.9044735),\n",
    "    (-117.1073262, 32.6184122),\n",
    "    (-114.2955756, 32.6554188),\n",
    "    (-114.1637748, 34.3047333),\n",
    "    (-114.7349117, 35.0995465),\n",
    "    (-120.0948112, 39.0254518),\n",
    "    (-120.0091050, 41.9727325),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542dc1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earthdata-client | \u001b[32m2025-10-04T18:50:15.398342-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 2 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1733.18it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:16<00:00, 16.38s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earthdata-client | \u001b[32m2025-10-04T18:50:31.807162-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Opening file: /var/folders/78/l07nwp291d17m9j1z3ylv1n40000gn/T/earthdata_oxnu76kf/TEMPO_NO2_L3_V03_20250101T144826Z_S005.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3890.82it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:16<00:00, 16.46s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16070.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earthdata-client | \u001b[32m2025-10-04T18:50:53.606690-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Opening file: /var/folders/78/l07nwp291d17m9j1z3ylv1n40000gn/T/earthdata_oxnu76kf/TEMPO_NO2_L3_V03_20250101T144826Z_S005.nc\n",
      "earthdata-client | \u001b[32m2025-10-04T18:50:53.608151-0500\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | Failed to open dataset: /var/folders/78/l07nwp291d17m9j1z3ylv1n40000gn/T/earthdata_oxnu76kf/TEMPO_NO2_L3_V03_20250101T144826Z_S005.nc\n",
      "earthdata-client | \u001b[32m2025-10-04T18:50:53.608700-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Opening file: /var/folders/78/l07nwp291d17m9j1z3ylv1n40000gn/T/earthdata_oxnu76kf/TEMPO_NO2_L3_V03_20250101T154826Z_S006.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>time</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7217793</th>\n",
       "      <td>32.630001</td>\n",
       "      <td>-117.129997</td>\n",
       "      <td>2025-01-01 14:48:44.028173568</td>\n",
       "      <td>4.162722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217794</th>\n",
       "      <td>32.630001</td>\n",
       "      <td>-117.110001</td>\n",
       "      <td>2025-01-01 14:48:44.028173568</td>\n",
       "      <td>4.162722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217795</th>\n",
       "      <td>32.630001</td>\n",
       "      <td>-117.089996</td>\n",
       "      <td>2025-01-01 14:48:44.028173568</td>\n",
       "      <td>4.360855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217796</th>\n",
       "      <td>32.630001</td>\n",
       "      <td>-117.070000</td>\n",
       "      <td>2025-01-01 14:48:44.028173568</td>\n",
       "      <td>4.162722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7217797</th>\n",
       "      <td>32.630001</td>\n",
       "      <td>-117.050003</td>\n",
       "      <td>2025-01-01 14:48:44.028173568</td>\n",
       "      <td>4.162722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          latitude   longitude                          time    weight\n",
       "7217793  32.630001 -117.129997 2025-01-01 14:48:44.028173568  4.162722\n",
       "7217794  32.630001 -117.110001 2025-01-01 14:48:44.028173568  4.162722\n",
       "7217795  32.630001 -117.089996 2025-01-01 14:48:44.028173568  4.360855\n",
       "7217796  32.630001 -117.070000 2025-01-01 14:48:44.028173568  4.162722\n",
       "7217797  32.630001 -117.050003 2025-01-01 14:48:44.028173568  4.162722"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.services.earth_data import EarthDataClient\n",
    "\n",
    "eac = EarthDataClient()\n",
    "df = eac.get_data(\n",
    "    dataset_name=short_name,\n",
    "    dataset_version=version,\n",
    "    start_date=date_start,\n",
    "    end_date=date_end,\n",
    "    polygon=polygon_coords,\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0388769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records retrieved: 214348\n",
      "Total records after dropping NaNs: 214348\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total records retrieved: {len(df)}\")\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Total records after dropping NaNs: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13d9a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "latitude     0\n",
       "longitude    0\n",
       "time         0\n",
       "weight       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134f8fc",
   "metadata": {},
   "source": [
    "## Load data to NO2 historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0944b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2024-04-20 00:00:00 to 2024-04-20 23:59:59Processing data for 2024-04-21 00:00:00 to 2024-04-21 23:59:59\n",
      "\n",
      "Processing data for 2024-04-22 00:00:00 to 2024-04-22 23:59:59\n",
      "Processing data for 2024-04-23 00:00:00 to 2024-04-23 23:59:59\n",
      "Processing data for 2024-04-24 00:00:00 to 2024-04-24 23:59:59\n",
      "Processing data for 2024-04-25 00:00:00 to 2024-04-25 23:59:59\n",
      "Processing data for 2024-04-26 00:00:00 to 2024-04-26 23:59:59\n",
      "Processing data for 2024-04-27 00:00:00 to 2024-04-27 23:59:59\n",
      "Processing data for 2024-04-28 00:00:00 to 2024-04-28 23:59:59\n",
      "Processing data for 2024-04-29 00:00:00 to 2024-04-29 23:59:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [00:05<00:58,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2024-04-30 00:00:00 to 2024-04-30 23:59:59Processing data for 2024-04-31 00:00:00 to 2024-04-31 23:59:59\n",
      "Error for 2024-04-24 00:00:00: 'Google' object has no attribute 'bigquery'\n",
      "\n",
      "Error for 2024-04-23 00:00:00: 'Google' object has no attribute 'bigquery'\n",
      "Error for 2024-04-26 00:00:00: 'Google' object has no attribute 'bigquery'\n",
      "Error for 2024-04-28 00:00:00: 'Google' object has no attribute 'bigquery'\n",
      "Error for 2024-04-27 00:00:00: 'Google' object has no attribute 'bigquery'\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:57.978934-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery client initialized (project=nasa-challenge-2025, location=US)\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:57.980221-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-29 00:00:00 to 2024-04-29 23:59:59\n",
      "Error for 2024-04-22 00:00:00: 'Google' object has no attribute 'bigquery'\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:57.989639-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-25 00:00:00 to 2024-04-25 23:59:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:58.315223-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-20 00:00:00 to 2024-04-20 23:59:59\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:58.342555-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-21 00:00:00 to 2024-04-21 23:59:59\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:58.632623-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-30 00:00:00 to 2024-04-30 23:59:59\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:59.297584-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 14 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2066.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:59.310830-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 14 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3533.53it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:59.334917-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 14 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4369.07it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:59.360719-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 14 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2178.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:59.409342-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 9 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3028.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:00:59.664589-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-31 00:00:00 to 2024-04-31 23:59:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [00:07<00:04,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for 2024-04-31 00:00:00: day is out of range for month: 2024-04-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:25<00:00, 25.09s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 17476.27it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2723.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:25<00:00, 25.53s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 14665.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1960.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:26<00:00, 26.31s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 7639.90it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1302.17it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:28<00:00, 28.12s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 12671.61it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1789.38it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:31<00:00, 31.12s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 3597.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1400.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:21<00:00, 21.85s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1891.03it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:22<00:00, 22.52s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 33554.43it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4777.11it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:23<00:00, 23.87s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 14122.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2298.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:19<00:00, 19.89s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 27060.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1903.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:23<00:00, 24.00s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 13148.29it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2376.38it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:35<00:00, 35.39s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 33288.13it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4165.15it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:39<00:00, 39.86s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 26379.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2933.08it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:40<00:00, 40.67s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1789.38it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:39<00:00, 39.64s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 28532.68it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3472.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:44<00:00, 44.54s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 6096.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1169.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:34<00:00, 34.66s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2028.19it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:35<00:00, 35.04s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1562.71it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:35<00:00, 35.81s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2325.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:43<00:00, 43.46s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16320.25it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2493.64it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:32<00:00, 32.85s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 31068.92it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1375.63it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:36<00:00, 36.93s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2922.86it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:36<00:00, 36.70s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2201.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:11<00:00, 71.97s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1218.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:34<00:00, 34.64s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 20867.18it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3269.14it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:22<00:00, 22.73s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 12052.60it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3075.00it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:37<00:00, 37.31s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2425.86it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:37<00:00, 37.18s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 20867.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3609.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:41<00:00, 41.57s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3300.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:38<00:00, 38.63s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16912.52it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1295.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:29<00:00, 29.77s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 9489.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2368.33it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:59<00:00, 59.01s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 21399.51it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3744.91it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:33<00:00, 33.51s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2709.50it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:33<00:00, 33.31s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 7423.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1766.77it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:31<00:00, 31.36s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4975.45it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:35<00:00, 35.91s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1243.86it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:36<00:00, 36.56s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16912.52it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2993.79it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:40<00:00, 40.15s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 24528.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2487.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:32<00:00, 32.95s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3344.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:37<00:00, 37.46s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2007.80it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:33<00:00, 33.79s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 32263.88it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2353.71it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:36<00:00, 36.70s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 28532.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:06:05.100720-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Downloaded 9 files. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:10<00:00, 70.33s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 26546.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2571.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:08<00:00, 68.91s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 3650.40it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1377.44it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:20<00:00, 80.22s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 28532.68it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2624.72it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:06:53.657864-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracted 964566 records from TEMPO_NO2_L3 version V03.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:06:54.237049-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data cleaned. 964566 records remaining after cleaning.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:06:54.237643-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Uploading data to BigQuery...\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:06:55.625102-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Dataset earth_data already exists (project=nasa-challenge-2025).\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:06:55.626951-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery load configured to WRITE_APPEND and CREATE_IF_NEEDED\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:07:02.607087-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data uploaded successfully.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:07:02.609251-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data successfully loaded into BigQuery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [06:09<04:17, 64.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success for 2024-04-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:32<00:00, 32.19s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2165.36it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:19<00:00, 79.56s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 27235.74it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4128.25it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:31<00:00, 31.25s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16194.22it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2150.93it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:31<00:00, 31.19s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 28532.68it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3968.12it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:23<00:00, 23.27s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2563.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:38<00:00, 38.08s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 15363.75it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1100.29it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:42<00:00, 42.58s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1795.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:19<00:00, 19.99s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1490.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:44<00:00, 44.77s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 13842.59it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2799.94it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:17<00:00, 17.75s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 2416.07it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:19<00:00, 19.34s/it]\n",
      "\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 779.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 451.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:19<00:00, 19.35s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 3705.22it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4002.20it/s]\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:25<00:00, 25.94s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 25731.93it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 3134.76it/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:17<00:00, 17.64s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1302.17it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:15<00:00, 15.92s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:08:39.720419-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Downloaded 14 files. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:57<00:00, 57.37s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 22671.91it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1138.83it/s]\n",
      "\n",
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:12<00:00, 72.16s/it]\n",
      "\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 23831.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:09:45.484081-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Downloaded 14 files. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:09:55.985989-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracted 1500436 records from TEMPO_NO2_L3 version V03.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:10:00.440367-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data cleaned. 1500436 records remaining after cleaning.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:10:00.441487-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Uploading data to BigQuery...\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:10:01.782596-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Dataset earth_data already exists (project=nasa-challenge-2025).\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:10:04.030092-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery load configured to WRITE_APPEND and CREATE_IF_NEEDED\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:10:16.927249-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data uploaded successfully.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:10:16.928368-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data successfully loaded into BigQuery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [09:24<04:31, 90.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success for 2024-04-21 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [02:28<00:00, 148.62s/it]\n",
      "\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 1275.25it/s]\n",
      "\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 266.37it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:08.071038-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracted 1500436 records from TEMPO_NO2_L3 version V03.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:08.900638-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data cleaned. 1500436 records remaining after cleaning.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:08.901171-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Uploading data to BigQuery...\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:09.216956-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Dataset earth_data already exists (project=nasa-challenge-2025).\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:09.219110-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery load configured to WRITE_APPEND and CREATE_IF_NEEDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:51<00:00, 111.58s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 21399.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:09.380426-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Downloaded 14 files. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:17.519960-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data uploaded successfully.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:11:17.520508-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data successfully loaded into BigQuery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [10:24<02:47, 83.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success for 2024-04-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [01:17<00:00, 77.56s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 27594.11it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1088.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:18.807236-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracted 1500436 records from TEMPO_NO2_L3 version V03.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:19.581745-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data cleaned. 1500436 records remaining after cleaning.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:19.582174-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Uploading data to BigQuery...\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:19.823110-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Dataset earth_data already exists (project=nasa-challenge-2025).\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:19.824357-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery load configured to WRITE_APPEND and CREATE_IF_NEEDED\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:26.074368-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data uploaded successfully.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:26.075717-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data successfully loaded into BigQuery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [11:33<01:20, 80.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success for 2024-04-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:29<00:00, 29.84s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 11096.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:12:42.154952-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Downloaded 14 files. Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:04.984290-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracted 1500436 records from TEMPO_NO2_L3 version V03.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:05.814221-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data cleaned. 1500436 records remaining after cleaning.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:05.814934-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Uploading data to BigQuery...\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:06.046719-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Dataset earth_data already exists (project=nasa-challenge-2025).\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:06.049453-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery load configured to WRITE_APPEND and CREATE_IF_NEEDED\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:12.910791-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data uploaded successfully.\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T11:14:12.911907-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data successfully loaded into BigQuery.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [13:20<00:00, 66.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success for 2024-04-20 00:00:00\n",
      "ETL process completed in 800.32 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from src.etl.extract_load_no2 import extract_and_load_no2\n",
    "\n",
    "short_name = \"TEMPO_NO2_L3\"  # collection name to search for in the EarthData\n",
    "version = \"V03\"\n",
    "\n",
    "polygon_coords = [\n",
    "    (-120.0091050, 41.9727325),\n",
    "    (-124.6045661, 41.8898826),\n",
    "    (-120.4462801, 33.9044735),\n",
    "    (-117.1073262, 32.6184122),\n",
    "    (-114.2955756, 32.6554188),\n",
    "    (-114.1637748, 34.3047333),\n",
    "    (-114.7349117, 35.0995465),\n",
    "    (-120.0948112, 39.0254518),\n",
    "    (-120.0091050, 41.9727325),\n",
    "]\n",
    "\n",
    "def process_day(i):\n",
    "    date_start = f\"2024-04-{i:02d} 00:00:00\"\n",
    "    date_end = f\"2024-04-{i:02d} 23:59:59\"\n",
    "    print(f\"Processing data for {date_start} to {date_end}\")\n",
    "    try:\n",
    "        extract_and_load_no2(\n",
    "            dataset_name=short_name,\n",
    "            dataset_version=version,\n",
    "            start_date=date_start,\n",
    "            end_date=date_end,\n",
    "            polygon=polygon_coords,\n",
    "        )\n",
    "        return f\"Success for {date_start}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error for {date_start}: {e}\"\n",
    "\n",
    "start_run_time = time.time()\n",
    "\n",
    "# Run in parallel using ThreadPoolExecutor\n",
    "days = list(range(20, 32))\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:  # Adjust max_workers as needed\n",
    "    futures = [executor.submit(process_day, i) for i in days]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(days)):\n",
    "        result = future.result()\n",
    "        print(result)\n",
    "\n",
    "end_run_time = time.time()\n",
    "elapsed_time = end_run_time - start_run_time\n",
    "print(f\"ETL process completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016e3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2024-04-01 00:00:00 to 2024-04-01 23:59:59\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T09:25:07.301841-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery client initialized (project=nasa-challenge-2025, location=US)\n",
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T09:25:07.302898-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Extracting data for TEMPO_NO2_L3 version V03 from 2024-04-01 00:00:00 to 2024-04-01 23:59:59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.etl.extract_load_no2 | \u001b[32m2025-10-05T09:25:08.391120-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Found 13 granules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 877.84it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:26<00:00, 26.55s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 39945.75it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1933.75it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:22<00:00, 22.86s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1748.36it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:28<00:00, 28.32s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16578.28it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1660.45it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:38<00:00, 38.82s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 24385.49it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:37<00:00, 37.82s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1644.83it/s]\n",
      "PROCESSING TASKS | : 100%|██████████| 1/1 [00:41<00:00, 41.63s/it]\n",
      "COLLECTING RESULTS | : 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "QUEUEING TASKS | : 100%|██████████| 1/1 [00:00<00:00, 1546.00it/s]\n",
      "PROCESSING TASKS | :   0%|          | 0/1 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m     date_end = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m2024-04-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 23:59:59\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_end\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     _ = \u001b[43mextract_and_load_no2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshort_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpolygon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolygon_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m end_run_time = time.time()\n\u001b[32m     36\u001b[39m elapsed_time = end_run_time - start_run_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/src/etl/extract_load_no2.py:24\u001b[39m, in \u001b[36mextract_and_load_no2\u001b[39m\u001b[34m(dataset_name, dataset_version, start_date, end_date, polygon)\u001b[39m\n\u001b[32m     20\u001b[39m google = Google()\n\u001b[32m     21\u001b[39m logger.info(\n\u001b[32m     22\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracting data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m df = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolygon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m logger.info(\n\u001b[32m     26\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/src/services/earth_data/client.py:132\u001b[39m, in \u001b[36mEarthDataClient.get_data\u001b[39m\u001b[34m(self, dataset_name, dataset_version, start_date, end_date, polygon)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m granule \u001b[38;5;129;01min\u001b[39;00m search_results:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m         dl = \u001b[43mea\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgranule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m         file_paths.append(\u001b[38;5;28mstr\u001b[39m(dl[\u001b[32m0\u001b[39m]))\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mRuntimeError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/earthaccess/api.py:420\u001b[39m, in \u001b[36mdownload\u001b[39m\u001b[34m(granules, local_path, provider, threads, show_progress, credentials_endpoint, pqdm_kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m     granules = [granules]\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mearthaccess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__store__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgranules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    430\u001b[39m     logger.error(\n\u001b[32m    431\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: You must call earthaccess.login() before you can download data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/earthaccess/store.py:652\u001b[39m, in \u001b[36mStore.get\u001b[39m\u001b[34m(self, granules, local_path, provider, threads, credentials_endpoint, show_progress, pqdm_kwargs)\u001b[39m\n\u001b[32m    643\u001b[39m     show_progress = _is_interactive()\n\u001b[32m    645\u001b[39m pqdm_kwargs = {\n\u001b[32m    646\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexception_behaviour\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mimmediate\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# should be overridden by pqdm_kwargs if passed\u001b[39;00m\n\u001b[32m    647\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_jobs\u001b[39m\u001b[33m\"\u001b[39m: threads,\n\u001b[32m    648\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;129;01mnot\u001b[39;00m show_progress,\n\u001b[32m    649\u001b[39m     **(pqdm_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    650\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgranules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/multimethod/__init__.py:350\u001b[39m, in \u001b[36mmultimethod.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m func = \u001b[38;5;28mself\u001b[39m.dispatch(*args)\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DispatchError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFunction \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__code__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/earthaccess/store.py:800\u001b[39m, in \u001b[36mStore._get_granules\u001b[39m\u001b[34m(self, granules, local_path, provider, credentials_endpoint, pqdm_kwargs)\u001b[39m\n\u001b[32m    795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    798\u001b[39m     \u001b[38;5;66;03m# if the data are cloud-based, but we are not in AWS,\u001b[39;00m\n\u001b[32m    799\u001b[39m     \u001b[38;5;66;03m# it will be downloaded as if it was on prem\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_onprem_granules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/earthaccess/store.py:901\u001b[39m, in \u001b[36mStore._download_onprem_granules\u001b[39m\u001b[34m(self, urls, directory, pqdm_kwargs)\u001b[39m\n\u001b[32m    890\u001b[39m arguments = [(url, directory) \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls]\n\u001b[32m    892\u001b[39m pqdm_kwargs = {\n\u001b[32m    893\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexception_behaviour\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mimmediate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    894\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdisable\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    898\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33margument_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    899\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/pqdm/threads.py:22\u001b[39m, in \u001b[36mpqdm\u001b[39m\u001b[34m(array, function, n_jobs, argument_type, bounded, exception_behaviour, tqdm_class, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpqdm\u001b[39m(\n\u001b[32m     13\u001b[39m     array: Iterable[Any],\n\u001b[32m     14\u001b[39m     function: Callable[[Any], Any],\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     **kwargs\n\u001b[32m     21\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_process\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43margument_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43margument_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBoundedThreadPoolExecutor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbounded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_behaviour\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexception_behaviour\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/pqdm/_base.py:65\u001b[39m, in \u001b[36m_parallel_process\u001b[39m\u001b[34m(iterable, function, n_jobs, executor, argument_type, exception_behaviour, tqdm_class, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m     processing_opts[\u001b[33m'\u001b[39m\u001b[33mdesc\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mPROCESSING TASKS | \u001b[39m\u001b[33m'\u001b[39m + processing_opts.get(\u001b[33m'\u001b[39m\u001b[33mdesc\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     63\u001b[39m     processing_opts[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(futures)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprocessing_opts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[32m     68\u001b[39m collecting_opts = copy.copy(tqdm_opts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from src.etl.extract_load_no2 import extract_and_load_no2\n",
    "\n",
    "short_name = \"TEMPO_NO2_L3\"  # collection name to search for in the EarthData\n",
    "# short_name = \"OMHCHOd\"\n",
    "version = \"V03\"\n",
    "# version = \"003\"\n",
    "\n",
    "date_start = \"2024-04-01 00:00:00\"\n",
    "date_end = \"2024-04-01 23:59:59\"\n",
    "polygon_coords = [\n",
    "    (-120.0091050, 41.9727325),\n",
    "    (-124.6045661, 41.8898826),\n",
    "    (-120.4462801, 33.9044735),\n",
    "    (-117.1073262, 32.6184122),\n",
    "    (-114.2955756, 32.6554188),\n",
    "    (-114.1637748, 34.3047333),\n",
    "    (-114.7349117, 35.0995465),\n",
    "    (-120.0948112, 39.0254518),\n",
    "    (-120.0091050, 41.9727325),\n",
    "]\n",
    "\n",
    "start_run_time = time.time()\n",
    "for i in range(1, 32):\n",
    "    date_start = f\"2024-04-{i:02d} 00:00:00\"\n",
    "    date_end = f\"2024-04-{i:02d} 23:59:59\"\n",
    "    print(f\"Processing data for {date_start} to {date_end}\")\n",
    "    _ = extract_and_load_no2(\n",
    "        dataset_name=short_name,\n",
    "        dataset_version=version,\n",
    "        start_date=date_start,\n",
    "        end_date=date_end,\n",
    "        polygon=polygon_coords,\n",
    "    )\n",
    "end_run_time = time.time()\n",
    "elapsed_time = end_run_time - start_run_time\n",
    "print(f\"ETL process completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdadc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04T17:12:05.948804-0500\u001b[0m | \u001b[1mINFO\u001b[0m | BigQuery client initialized (project=%s, location=%s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciorequena/Documents/PersonalProjects/NASA_Challenge_2025/data-operations/.venv/lib/python3.13/site-packages/google/auth/_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04T17:12:07.074801-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Dataset %s already exists (project=%s).\n",
      "\u001b[32m2025-10-04T17:12:11.427748-0500\u001b[0m | \u001b[1mINFO\u001b[0m | Data uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "google = Google()\n",
    "_ = google.bigquery.upload_data_from_dataframe(\n",
    "    df,\n",
    "    dataset=\"earth_data\",\n",
    "    table_id=\"no2_historical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e89ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
